{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your name: <type it here, please>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "In this homework, you will analyze the statistics of the Wikipedia's article about the famous match between a South Korean professional Go player and 18-time Go world champion Lee Sedol and AlphaGo, a machine learning system developed by Google DeepMind (https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol ). You will use the spaCy library (https://spacy.io/) for the purposes of text processing, analysis and visualization. You will learn how to extract individual sentences and words from raw text, to annotate them with part-of-speech tags, to detect entities and to build parse trees.\n",
    "\n",
    "Please follow the instructions in this notebook to complete the assignment.\n",
    "\n",
    "Submit your homework as follows:\n",
    "\n",
    "```sh\n",
    "$ submit arum hw1 <your_notebook_filename>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import libraries and load data\n",
    "Please make sure to put the input text file \"corpus.txt\" into the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have spacy and matplotlib installed\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the English model from spaCy. \n",
    "# You can read the documentation here (https://spacy.io/models/en#en_core_web_lg)\n",
    "# Make sure to run the \"$ python3 -m spacy download en_core_web_lg\" command in your terminal before executing this cell\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "with open('corpus.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Input text statistics\n",
    "This part of the homework is dedicated to tokenization. Tokenization is the process of segmenting text into individual linguistic units: characters, words, sentences, etc. Tokenization is the first (after data collection and cleaning) step of the text processing workflow. The goal of tokenization is, given input text, to return a sequence of tokens it consists of. We'll be using spaCy to look into how tokenization is done.\n",
    "\n",
    "Your task is to complete the missing code in the function `process_text`.\n",
    "In this step you will have to implement the following:\n",
    "- Tokenize the input text into sentences using spaCy sentence tokenization\n",
    "- Tokenize the input text into tokens and counting the frequency of each one of them. Note: your implementation should disregard punctuation tokens. You can use the `is_punct` attribute to check if a given token is a punctuation token.\n",
    "- Lemmatize the derived tokens if `lemmatize=True`\n",
    "- Lowecase the derived tokens if `lowercase=True`\n",
    "- Filter the most frequent tokens if their count exceeds `high_thr`\n",
    "- Filter the least frequent tokens if their count is below `low_thr`\n",
    "\n",
    "When finished, run the cells below to answer the questions. __For every cell make sure to call the completed function with the correct parameters.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, lemmatize=False, lowercase=False, high_thr=None, low_thr=None):\n",
    "    \"\"\"\n",
    "    Tokenize text at a sentence- and at a word-level. Punctuation is ignored.\n",
    "    Return a list of sentences and a dictionary of tokens' counts.\n",
    "    Args:\n",
    "        text (str): The input text\n",
    "        lemmatize (bool, optional): A flag for whether derived tokens should be lemmatized or not.\n",
    "                                    Note: spaCy's lemmatization also lowercases a given token\n",
    "        lowercase (bool, optional): A flag for whether derived tokens should be lowercased or not\n",
    "        high_thr (int, optional): The threshold frequency for cutting off the tokens if their count exceeds high_thr\n",
    "        low_thr (int, optional): The threshold frequency for cutting off the tokens if their count is below low_thr\n",
    "\n",
    "    Returns:\n",
    "        sentences (list): A list of sentences in the text.\n",
    "        tok_counter (dict): Maps tokens to their counts in the text ({'word': count}). \n",
    "                            If lemmatize=True, the returned dictionary's keys should be lemmatized.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize text into sentences\n",
    "    ### YOUR CODE BELOW ###\n",
    "    sentences = None\n",
    "    ### YOUR CODE ABOVE ###\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    ### YOUR CODE BELOW ###\n",
    "    tok_counter = None\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### YOUR CODE ABOVE ###\n",
    "    \n",
    "    # Filtration if high_thr or low_thr are set\n",
    "    if high_thr is not None:\n",
    "        ### YOUR CODE BELOW ###\n",
    "        pass\n",
    "        # change tok_counter here\n",
    "        ### YOUR CODE ABOVE\n",
    "        \n",
    "    if low_thr is not None:\n",
    "        ### YOUR CODE BELOW ###\n",
    "        pass\n",
    "        # change tok_counter here\n",
    "        #### YOUR CODE ABOVE ###\n",
    "    \n",
    "    return sentences, tok_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many sentences are there in the text? Print out the 61st sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the function input parameters if needed\n",
    "sentences, tok_counter = process_text(text)\n",
    "\n",
    "# Print the answer\n",
    "print(\"There are {} sentences in total.\\n\".format(len(sentences)))\n",
    "print(\"The 61st sentence is: \\n{}\".format(sentences[60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How many times is the word \"go\" used?\n",
    "Make sure your implementation is NOT case sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the function input parameters if needed\n",
    "sentences, tok_counter = process_text(text)\n",
    "\n",
    "# Print the answer\n",
    "print(\"The word 'go' is used {} times\".format(tok_counter[\"go\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How many times is any form of the word \"go\" used? \n",
    "(\"go\", \"went\", \"goes\", \"Going\", etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the function input parameters if needed\n",
    "sentences, tok_counter = process_text(text)\n",
    "\n",
    "# Print the answer\n",
    "print(\"The forms of the word 'go' are used {} times\".format(tok_counter[\"go\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How many tokens appear in the text no more than 20 times and no less than 5 times?\n",
    "Your implementation should be case-sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the function input parameters if needed\n",
    "sentences, tok_counter = process_text(text)\n",
    "\n",
    "# Print the answer\n",
    "print(\"There are {} tokens within the [5, 20] frequency range\".format(len(tok_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualize Zipf's law (https://en.wikipedia.org/wiki/Zipf%27s_law)\n",
    "Zipf's law shows how the probabilities of the words in the corpus are distributed.\n",
    "\n",
    "For the sake of visualization purposes (and speed) you can consider only tokens that appear in the text not less than 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the function input parameters if needed\n",
    "sentences, tok_counter = process_text(text)\n",
    "\n",
    "# Plot the output\n",
    "tok_counter = sorted(tok_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "words = [tok[0] for tok in tok_counter]\n",
    "counts = [tok[1] for tok in tok_counter]\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.bar(words, counts)\n",
    "plt.xticks(rotation=70, fontsize=12)\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part of speech tagging\n",
    "Part of speech (POS) tags help to understand the role of a word in a sentence. Knowing a given word's part of speech provides information about overall syntactic structure of a sentence and helps to disambiguate the meaning of the word. SpaCy has built-in automatic POS tagging, which you'll be using in this exercise. As in the previous function, the punctuation should be ignored in this task.\n",
    "\n",
    "Your task is to complete the missing code in the function `get_pos` and in the cells below. In this step you will have to implement the following:\n",
    "\n",
    "- Detect part of speech of every (non-pucntuation-)token in the text\n",
    "- Construct a dictionary of POS tags and corresponding words. The keys of the dictionary are POS tags, and the value are list of all the words with this tag\n",
    "- Similarly, construct a dictionary where the keys are lemmatized words and values are lists of all detected POS tags\n",
    "\n",
    "You might find the `defaultdict` class from the `collections` library useful for this task.\n",
    "\n",
    "The two dictionaries will be used for further analysis.\n",
    "When finished, __complete the code in the cells below__ to answer the questions. Make sure to call the completed function with the correct input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(text, lemmatize=False):\n",
    "    \"\"\"\n",
    "    Build a mapping between every token (ignoring punctuation) and every detected POS tag within the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        lemmatization (bool, optional): A flag for whether or not tokens should be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "        token2pos (dict): The token-to-list of corresponding POS tags mapping ({'cotton':['NOUN', 'ADJ']})\n",
    "        pos2token (dict): The POS_tag-to-list of corresponding lemmatized tokens mapping ({'NOUN':['apple', 'pear']})\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    ### YOUR CODE BELOW ###\n",
    "    token2pos = None\n",
    "    pos2token = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### YOUR CODE ABOVE ###\n",
    "        \n",
    "    return token2pos, pos2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  How many adjectives are there in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "token2pos, pos2token = None, None\n",
    "num_adjectives = None\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "# Print output\n",
    "print(\"There are {} adjectives in the text.\".format(num_adjectives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the 3rd most popular verb in the text?\n",
    "Note: only verbs are considered in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "token2pos, pos2token = None, None\n",
    "verbs_cnt = None\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "verbs_sorted = sorted(verbs_cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print output\n",
    "print(\"The 3rd most popular verb is '{}'\".format(verbs_sorted[2][0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. If you only count the canonical forms of words, what is the 3rd most popular verb form in the text?\n",
    "Note: \"go\", \"going\", \"went\", etc. refer to the same canonical form \"go\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "token2pos, pos2token = None, None\n",
    "verb_forms_cnt = None\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "verb_forms_sorted = sorted(verb_forms_cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print output\n",
    "print(\"The 3rd most popular verb is 'to {}'\".format(verb_forms_sorted[2][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Which words are the most \"ambiguous\"? (i.e. which words have more than 2 syntactic roles depending on the context?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NER analysis\n",
    "Named entity recognition (NER) is an important information extraction task in NLP. The goal of NER is to identify named entities (such as places, people, organizations, etc.) in unstructured text. NER helps to avoid ambiguity, resolve coreferences, represent the meaning, and find relations between individual documents. It is widely used for question answering, news searching, textual entailment, and other tasks in NLP. As you might have guessed, spaCy automatically detects named entities in a given text, which you will implement in this exercise.\n",
    "\n",
    "\n",
    "Your task is to complete the missing code in the function `get_entities`. In this step you will have to implement the following:\n",
    "\n",
    "- detect all the entities in the text\n",
    "- for every entity type store all the detected tokens of this type\n",
    "\n",
    "Note that in this case the input text shouldn't be lowercased, as capital letters help with detecting named entities.\n",
    "\n",
    "When finished, __complete the code in the cells below__ to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    \"\"\"\n",
    "    Build a mapping between entity labels and tokens with the assigned labels.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        entity_dict (dict): The dictionary containing entity labels as keys and lists of corresponding tokens as values.\n",
    "                            {'PERSON':['Barack Obama', 'George Bush']} \n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    ### YOUR CODE BELOW ###\n",
    "    entity_dict = dict()\n",
    "\n",
    "    \n",
    "    ### YOUR CODE ABOVE ###\n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict = get_entities(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many unique labels are detected in the text? What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "labels = None\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "# Print output\n",
    "print(\"There are {} unique labels identified\".format(len(labels)))\n",
    "print(list(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How many dates are detected in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "date_cnt = None\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "# Print output\n",
    "print(\"There are {} dates identified\".format(date_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize all the entities detected in the first five sentences.\n",
    "Use `displacy.render` to visualize how the entities of the first 5 sentences in the text are detected. You can use the detected sentences from the 1st part of the homework.\n",
    "See https://spacy.io/usage/visualizers for reference.\n",
    "__Note__: for proper visualization, it is recommended that you first convert all the sentences to the `str` type, convert the obtained joined text sentences back to the `Doc` type, and then call the `displacy.render()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "displacy.render()\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Dependency parsing\n",
    "Dependency parsing helps identifying the grammatical structure of a sentence, establishing dependency relations between single words. Dependency parsing have been shown to improve NLP systems in certain languages; the best long paper in the top NLP conference EMNLP-2018 integrated syntactic parsing in their system (https://arxiv.org/pdf/1804.08199.pdf). Although there are a lot of methods to incorporate syntactic features in a model, we will look at spaCy's parser in this part of the homework.\n",
    "\n",
    "This step doesn't require you to complete a function. Instead, you will visualize the parse tree reflecting dependency relations in the 61st sentence from the text (refer to the first part of the homework). You should use `displacy.render` class to complete this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(sentences[60])\n",
    "\n",
    "### YOUR CODE BELOW ###\n",
    "displacy.render()\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
