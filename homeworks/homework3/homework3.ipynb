{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this homework you will see what [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) are, how to train them and how to evaluate them. Word embeddings are widely used in practice and you will most likely use them in your projects. Keep in mind that training a neural network can take time, so before running the notebook please test your implementation on a smaller subset of your dataset.\n",
    "\n",
    "__You will learn:__\n",
    "- To implement your own neural network that trains word embeddings (using the code from the previous homework)\n",
    "- Learn how to run intrinsic evaluations on the obtained embeddings\n",
    "- Compare your own embeddings to the pre-trained ones\n",
    "\n",
    "__You will need:__\n",
    "\n",
    "- To install the `gensim` package as `pip install gensim`\n",
    "- Access to GPUs\n",
    "- The code from your second homework\n",
    "\n",
    "Once you complete this assignment, submit it as:\n",
    "\n",
    "`submit arum hw3 <name_of_your_notebook>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that execution of this cell doesn't return any errors. If it does, go the class repository and follow the environment setup instructions\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk import word_tokenize\n",
    "\n",
    "% matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skip-gram model is a subtype of the [Word2Vec model ](https://en.wikipedia.org/wiki/Word2vec) that aims to predict context words given a target word. In the previous homework, you implemented all the necessary classes to feed the data in the skip-gram model. In this step, you will implement the model architecture and the training loop. You will then put everything together to train the skip-gram model, save the trained word embeddings, and evaluate them.\n",
    "\n",
    "The high-level overview of this part of the homework is as follows:\n",
    "\n",
    "- Complete the `SkipGramModel` class:\n",
    "    * Initialize the layers\n",
    "    * Complete the `forward()` method\n",
    "    * Complete the `save_embeddings()` method\n",
    "- Prepare the data. Use the code from the previous homework to:\n",
    "    * Tokenize the input text\n",
    "    * Construct and prune the vocabulary\n",
    "    * Initialize the `SkipGramDataset` class\n",
    "    * Initialize the `DataLoader` class\n",
    "    * Initialize the model class\n",
    "- Complete the training loop:\n",
    "    * Fetch the input batches from the dataloader\n",
    "    * Run the inputs through the model and get the predictions\n",
    "    * Calculate the loss\n",
    "    * Backpropagate the error\n",
    "    * Update the weights\n",
    "    * Save the embeddings\n",
    "- Visualize the learning curve\n",
    "\n",
    "__Insert the code for the `preprocess()` function, the `Vocabulary` class, and the `SkipGramDataset` class in the corresponding cells below and execute those cells. Follow the instructions below to complete this part of the homework.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models implemented in PyTorch should subclass the [`torch.nn.Module` class](https://pytorch.org/docs/stable/nn.html?highlight=module#torch.nn.Module). The main method of this class (which is used by a lot of other PyTorch classes) is `forward()`. `forward()` is the core method that defines how your model is going to run and what outputs it should produce given the inputs. \n",
    "In the constructor of the your model class you should initialize all the layers you are going to use. The skip-gram model is conceptually simple - it stores embeddings in a trainable matrix and projects the target word's embedding to a vector of the vocabulary size for making a prediction about the context word. Your task is to complete the code in the `SkipGramModel` class below. Please refer to the [documentation of PyTorch](https://pytorch.org/docs/stable/nn.html) to correctly initialize the layers.\n",
    "\n",
    "__Instructions__:\n",
    "\n",
    "- Complete the class constructor\n",
    "    * Initialize the `nn.Embedding` layer\n",
    "    * Initialize the `nn.Linear` projection layer\n",
    "- Complete the `forward()` method that takes a tensor of the shape [Bx1] and returns a prediction vector of the shape [BxV], where B is the batch size and V is the vocabulary size.\n",
    "- Complete the `save_embeddings()` method that extracts the weights of the embedding layers and saves them to a file. The format of the file should be as follows:\n",
    "\n",
    "```\n",
    "<number_of_words> <dimension>\n",
    "<word1>                <num_1> <num_2> ... <num_dimension>\n",
    "<word2>                <num_1> <num_2> ... <num_dimension>\n",
    "...\n",
    "<word_number_of_words> <num_1> <num_2> ... <num_dimension>\n",
    "```\n",
    "For more details, please refer to the section 2 of this notebook.\n",
    "\n",
    "__Notes__:\n",
    "\n",
    "- The weights of a layer can be extracted using the `weight` attribute.\n",
    "- To convert `torch.Tensor` to a `numpy` array you first need to detach it from the current computational graph and then transfer it to CPU, which can be done using a chain of the `.detach()` and the `.cpu()` methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): vocabulary size\n",
    "            embedding_dim (int): the dimension of word embeddings\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "\n",
    "        \n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.LongTensor): input tensor containing batches of word ids [Bx1]\n",
    "        Returns:\n",
    "            outputs (torch.FloatTensor): output tensor with unnormalized probabilities over the vocabulary [BxV]\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        outputs = None\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        return outputs\n",
    "    \n",
    "    def save_embeddings(self, voc, path):\n",
    "        \"\"\"\n",
    "        Save the embedding matrix to a specified path.\n",
    "        \n",
    "        Args:\n",
    "            voc (Vocabulary): the Vocabulary object for id-to-token mapping\n",
    "            path (str): the location of the target file\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeddings = None\n",
    "        with open(path, 'w') as f:\n",
    "            pass\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        print(\"Successfuly saved to {}\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below reads the data, initializes all the classes and parameters necessary for training. Please carefully read through, understand, and run the code. \n",
    "\n",
    "__Notes__:\n",
    "\n",
    "- We are going to use the [CrossEntropy loss](https://en.wikipedia.org/wiki/Cross_entropy) which is a standard loss function for classification tasks. For numeric stability, [PyTorch implementation of the cross entropy function](https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss) combines the log softmax function and the negative loglikelihood loss computation. Because of this, you don't need to apply the softmax function to the output of your model.\n",
    "\n",
    "- We are going to use the [Adagrad optimizer](https://pytorch.org/docs/stable/optim.html?highlight=adagrad#torch.optim.Adagrad) for gradient descent optimization.\n",
    "\n",
    "- For the sake of speed, we encourage you to keep the values of the parameters as they are. However, after you complete the assignment, feel free to change them to see what effect this would have on the model's performance (but keep in mind that training can take much longer).\n",
    "\n",
    "- It is always a good idea to shuffle the dataset (think why)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING #\n",
    "with open('text8.txt') as f:\n",
    "    data = f.read()\n",
    "tokens = preprocess(data[:1000000])\n",
    "\n",
    "# CONSTRUCTING VOCABULARY #\n",
    "voc = Vocabulary()\n",
    "voc.add_tokens(tokens)\n",
    "voc.prune(5)\n",
    "vocab_size = len(voc)\n",
    "\n",
    "# TRAINING PARAMETERS #\n",
    "embedding_dim = 128\n",
    "skip_window = 2\n",
    "batch_size = 512\n",
    "lr = 0.1\n",
    "num_epochs = 100\n",
    "report_every = 5\n",
    "\n",
    "# DATASET\n",
    "dataset = SkipGramDataset(tokens, voc, skip_window=skip_window)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# MODEL\n",
    "model = SkipGramModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model and the dataset are ready, we will be looking at the training loop structure.\n",
    "Essentially, training of any neural network is an iterative process of (a) sampling a batch of training examples, (b) running them through the model, (c) computing the loss based on the predictions and the target labels, (d) performing the backward pass and computing the gradients, and (e) updating the model's weights. Keeping this logic in mind, complete the code in the cell below. \n",
    "\n",
    "__! Important__:\n",
    "Because PyTorch accumulates the gradients on subsequent backward passes, you will need to manually set the gradients to zero at every iteration to correctly update the parameters of your model. You can set the gradients to zero using the `zero_grad()` method of the used optimizer (in our case, Adagrad). \n",
    "\n",
    "__Instructions__:\n",
    "Complete the code below using the following methods:\n",
    "- [`zero_grad()`](https://pytorch.org/docs/stable/optim.html?highlight=zero_grad#torch.optim.Optimizer.zero_grad) and [`step()`](https://pytorch.org/docs/stable/optim.html?highlight=step#torch.optim.Optimizer.step) methods for optimizers\n",
    "- [`backward()`](https://pytorch.org/docs/stable/autograd.html?highlight=backward#torch.autograd.backward) method for the loss\n",
    "- [`item()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item) method for converting a `torch.Tensor` object of the size [1] to a float number\n",
    "- `cuda()` method for transferring tensors to GPUs\n",
    "\n",
    "__Notes__:\n",
    "\n",
    "- You can always verify if your model is training correctly - the loss should be decreasing. Please keep track of your epoch losses, since you will need them to plot the learning curve later.\n",
    "\n",
    "- __Warning__: The execution of the cell below will take time depending on the selected batch size, the number of epochs and the size of the dataset. If you are not sure your implementation is correct, test it on a small subset of the dataset (you can manually reduce the dataset by cutting off a portion of the dataset right after you load it) and using a small number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING #\n",
    "tick = time.time()\n",
    "epoch_losses = []\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    batch_losses = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        ### YOUR CODE BELOW ###\n",
    "        # Zero the gradients\n",
    "\n",
    "        # Extract the inputs and the targets\n",
    "        inputs, targets = None\n",
    "        \n",
    "        # Transfer the inputs and the targets to GPUs, if available\n",
    "        if torch.cuda.is_available():\n",
    "            pass\n",
    "\n",
    "        # Run the model\n",
    "        outputs = None\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = None\n",
    "        \n",
    "        # Backpropagate the error\n",
    "        \n",
    "        # Update the parameters\n",
    "\n",
    "        # Append the loss\n",
    "        batch_losses.append(None)\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "    epoch_loss = np.mean(np.array(batch_losses))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    if epoch_num % report_every == 0:\n",
    "        tock = time.time()\n",
    "        print(\"Epoch {}. Loss {:.4f}. Elapsed {:.0f} seconds\".format(epoch_num, epoch_loss, tock-tick))\n",
    "\n",
    "print(\"Total time elapsed: {:.0f} minutes\".format((tock-tick)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, save the weights of the embedding layer by calling the `save_embeddings()` method. For consistency, we suggest that you name your embeddings file as `skipgram_trained.vec`, because we will use this file later in the homework. If you decide to stick with a different name, make sure to make changes to the code in the next part of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results #\n",
    "### YOUR CODE BELOW ###\n",
    "\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learning curve of your model using the `matplotlib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "### YOUR CODE BELOW ###\n",
    "\n",
    "\n",
    "### YOUR CODE ABOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you trained your own word embeddings, you will evaluate and compare them to the publicly available pre-trained embeddings released by Facebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Loading the data\n",
    "\n",
    "In this part of the homework we will use pre-trained word embeddings and explore their properties. In particular, we will use [fastText](https://fasttext.cc/) embeddings trained on the Wikipedia data. The easiest way to use pre-trained embeddings is to download them from the official website and use the [gensim](https://radimrehurek.com/gensim/) library. This library allows you to easily load pre-trained embeddings from several widely used formats (such as .txt or binary) and perform simple operations on the loaded vectors. Some of the commonly used functions are calculating the similarity between two tokens and finding tokens that are most similar to the given one.\n",
    "\n",
    "Recall that Jupyter notebook allows executing shell commands directly from within the notebook. Anything appearing after `!` on a line will be executed not by the Python kernel, but by the system shell. \n",
    "\n",
    "__Instructions__:\n",
    "\n",
    "- Execute the cells below to download the embeddings file and make sure it was unzipped to your directory without any errors.\n",
    "\n",
    "__Note__:\n",
    "- The downloaded archive will take up 650 MB of your disk space; the unzipped file requires additional 2.2 GB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-clobber https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "!unzip -n wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh wiki-news-300d-1M*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `.vec` file is just a text file in a special format. Namely, the first line contains the total number of tokens and the size (length) of the embeddings separted by a space. In our case the first line is `999994 300`, which means that this file contains embeddings for 999994 unique tokens and the every embeddings is a 300-dimensional vector.\n",
    "The following lines contain the embeddings themselves in the format `<word1>  <num_1> <num_2> ... <num_dimension>`. Feel free to open it in your favorite text editor and to see what it looks like.\n",
    "\n",
    "Now let's load the embeddings using the `gensim` library. It contains a convenient `KeyedVectors` class that provides basic methods for working with embeddings. See the [documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) for details.\n",
    "After having loaded the embeddings you can perform different operations on them. In particular, you can obtain the embedding of a token through the indexing operation.\n",
    "\n",
    "__Instructions:__\n",
    "\n",
    "- Execute the cells below to see what the loaded vectors look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting an embedding of a single word\n",
    "v1 = word_vectors['cat']\n",
    "print(\"The dimension of the loaded embedding is {}\".format(v1.shape))\n",
    "print(\"The word 'cat' is represented by the following vector:\")\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of  word embeddings properties\n",
    "In this step you will explore the properties of word embeddings and look at what operations you can perform on word vectors. \n",
    "\n",
    "__Instructions:__\n",
    "\n",
    "- Complete the code in the cells below by calling the appropriate methods of the `KeyedVectors` class and execute the cells to answer the questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Find top 3 most similar in terms of cosine similarity words and their similarity to the word `cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "top3_most_similar = []\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "for word, similarity in top3_most_similar:\n",
    "    print('{}: {:.3f}'.format(word, similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Find the most similar word to the word `good`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "most_similar_to_good = []\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('The most similar word to the word \"good\" is the word \"{}\"'.format(most_similar_to_good[0]))\n",
    "print('The corresponding cosine similarity is {:.3f}'.format(most_similar_to_good[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think this happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__YOUR ANSWER HERE__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Find the cosine similarity between the words `woman` and `man`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "similarity = None\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('The cosine similarity between \"woman\" and \"man\" is {:.3f}'.format(similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vector arithmetics\n",
    "The original word2vec paper illustrates the properties of word embeddings through showing examples of analogies. Perhaps the most famous example is the `king::man` <=> `queen::woman` analogy.\n",
    "To make sure this analogy holds true in the embedding space, one would need to do the following:\n",
    "\n",
    " 1. Get the vectors for the words \"woman\", \"king\", and \"man\"\n",
    " 2. Add the vectors for \"woman\" and \"king\" together\n",
    " 3. Substract the vector for the word \"man\" from the result\n",
    " 4. Find the word with the closest embedding (likely one will end up with the word \"queen\")\n",
    " \n",
    "In this question we will explore a similar example on verb tenses and how they are captured in the learned word embeddings. \n",
    "\n",
    "__Instructions:__\n",
    "- Print the the closest word to the vector $\\text{walking} + \\text{swam} - \\text{swimming}$.\n",
    "\n",
    "__Note__: use vector arithmetics operations (`+` and `-`) and the method `similar_by_vector` of the `KeyedVectors` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word_vectors[\"walking\"]\n",
    "v2 = word_vectors[\"swam\"]\n",
    "v3 = word_vectors[\"swimming\"]\n",
    "\n",
    "### YOUR CODE BELOW ###\n",
    "v4_word = None\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('Resulting word: {}'.format(v4_word[0]))\n",
    "print('Similarity: {:.3f}'.format(v4_word[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing embeddings\n",
    "\n",
    "Using the same evaluation methods, you will now compare the embeddings you trained yourself with the pre-trained fastText embeddings. Execute the cell below to load the word vectors (change the names of the files if needed).\n",
    "\n",
    "__Note__:\n",
    "The execution of the cell below can take a while to finish (a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec', binary=False)\n",
    "skipgram = KeyedVectors.load_word2vec_format('skipgram_trained.vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Print out three most similar words (along with the similarities) to the word `money` for the both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "top3_similar_fasttext = []\n",
    "print(\"Pretrained model: \\n \", top3_similar_fasttext)\n",
    "top3_similar_skipgram = []\n",
    "print(\"Skip-gram model: \\n \", top3_similar_skipgram)\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 For the both models, compute the cosine similarities between the words `boy` and `man`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "fasttext_similarity = None\n",
    "print(\"Pretrained model: \\n \", fasttext_similarity)\n",
    "skipgram_similarity = None\n",
    "print(\"Skip-gram model: \\n \", skipgram_similarity)\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Can you suggest at least four ways in which the performance of your skip-gram model can be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
